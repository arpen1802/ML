{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"../../Data/yerevan_houses.csv\")\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "data.head()\n",
    "\n",
    "### Split into train test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data['area'] = data['area'].apply(pd.to_numeric,errors='coerce')\n",
    "\n",
    "data['price'] = data['price'].apply(pd.to_numeric,errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X = data['area']\n",
    "Y = data['price']\n",
    "X = X.values.reshape(-1,1)\n",
    "Y = Y.values.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=58)\n",
    "\n",
    "### Fit linear regression with area \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "### Plot fited line and points, calculate loss and R^2\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.scatter(X_test,y_test)\n",
    "plt.plot(X_test,reg.predict(X_test),'r')\n",
    "\n",
    "reg.score(X_test,y_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train, reg.predict(X_train))\n",
    "\n",
    "mean_squared_error(y_test,reg.predict(X_test))\n",
    "\n",
    "### Add new features, use pd.get_dumies for vectorization categorical variables\n",
    "\n",
    "data = data.drop(['url','region','street'],axis=1)\n",
    "\n",
    "\n",
    "data = data.drop(columns=['district'])\n",
    "# data['Condtion_vectorized']=pd.get_dummies(data['condition'])\n",
    "data_test = data\n",
    "data = pd.get_dummies(data,columns=None,drop_first=True)\n",
    "\n",
    "data.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['price']), data['price'], test_size=0.33, random_state=58)\n",
    "\n",
    "### Fit with new features, calculate loss and R^2\n",
    "\n",
    "reg1 = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "reg1.score(X_test,y_test)\n",
    "\n",
    "mean_squared_error(y_train, reg1.predict(X_train))\n",
    "\n",
    "mean_squared_error(y_test,reg1.predict(X_test))\n",
    "\n",
    "### Standartize features and fit again\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = ()\n",
    "\n",
    "data_dummies= set(data.columns).difference(data_test)\n",
    "\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "\n",
    "fit_x = StandardScaler().fit(X_train.drop(columns=set(data.columns).difference(data_test)))\n",
    "fit_y = StandardScaler().fit(y_train)\n",
    "X_scaled = fit_x.transform(X_train.drop(columns=set(data.columns).difference(data_test)))\n",
    "Y_scaled = fit_y.transform(y_train)\n",
    "X_scaled_test = fit_x.transform(X_test.drop(columns=set(data.columns).difference(data_test)))\n",
    "Y_scaled_test = fit_y.transform(y_test)\n",
    "\n",
    "pd.concat([pd.DataFrame(X_scaled,index=X_train.index), X_train[data_dummies]], axis=1)\n",
    "\n",
    "reg2 = LinearRegression().fit(pd.concat([pd.DataFrame(X_scaled,index=X_train.index), X_train[data_dummies]], axis=1), Y_scaled)\n",
    "\n",
    "reg2.score(pd.concat([pd.DataFrame(X_scaled_test,index=X_test.index), X_test[data_dummies]], axis=1),Y_scaled_test)\n",
    "\n",
    "### Try same with polynomial regression, monitor loss and choose best degree for polynomial regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "\n",
    "\n",
    "X_train_reg1 = poly.fit_transform(X_scaled)\n",
    "Y_train_reg1 = poly.fit_transform(Y_scaled)\n",
    "X_test_reg1 = poly.fit_transform(X_scaled_test)\n",
    "Y_test_reg1 = poly.fit_transform(Y_scaled_test)\n",
    "\n",
    "\n",
    "X_train_reg = poly.fit_transform(X_train)\n",
    "Y_train_reg = poly.fit_transform(y_train)\n",
    "X_test_reg = poly.fit_transform(X_test)\n",
    "Y_test_reg = poly.fit_transform(y_test)\n",
    "\n",
    "\n",
    "reg3 = LinearRegression().fit(X_train_reg, Y_train_reg)\n",
    "\n",
    "reg4 = LinearRegression().fit(X_train_reg1, Y_train_reg1)\n",
    "\n",
    "reg3.score(X_test_reg,Y_test_reg)\n",
    "\n",
    "mean_squared_error(Y_test_reg, reg3.predict(X_test_reg))\n",
    "\n",
    "reg4.score(X_test_reg1,Y_test_reg1)\n",
    "\n",
    "mean_squared_error(Y_test_reg1, reg4.predict(X_test_reg1))\n",
    "\n",
    "## After some tries I have figured out that both with scaled and not scaled data, polynomial regression works good with 1 degree. However, comparing scaled and non-scaled data, it can be seen that non-scaled data works better with polynomial regression that the scaled one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
